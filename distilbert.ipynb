{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    },
    "colab": {
      "name": "distilbert.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMZEiImFvbCy",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Classification Using DistilBERT "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2ZRiGpERSK1",
        "colab_type": "text"
      },
      "source": [
        "# What is `DistilBERT`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yzge4wUw08Q",
        "colab_type": "text"
      },
      "source": [
        "BERT is designed to pretrain deep bidirectional representations from\n",
        "unlabeled text by jointly conditioning on both\n",
        "left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer\n",
        "to create state-of-the-art models for a wide\n",
        "range of tasks, such as question answering and\n",
        "language inference, without substantial taskspecific architecture modifications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMEy6Ptaw1Ab",
        "colab_type": "text"
      },
      "source": [
        "DistilBERT is a small, fast, cheap and light Transformer model trained by distilling Bert base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of Bert’s performances as measured on the GLUE language understanding benchmark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL9_qWcSRYnZ",
        "colab_type": "text"
      },
      "source": [
        "# What is `ktrain`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-KPOCk1RYj2",
        "colab_type": "text"
      },
      "source": [
        "ktrain is a library to help build, train, debug, and deploy neural networks in the deep learning software framework, Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThRISEExRYgb",
        "colab_type": "text"
      },
      "source": [
        "# Notebook Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhnjIqyT4adZ",
        "colab_type": "code",
        "colab": {},
        "outputId": "b2577f3a-7621-4a03-e481-d2e048cdca28"
      },
      "source": [
        "!pip install ktrain"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ktrain\r\n",
            "  Downloading ktrain-0.18.4.tar.gz (25.2 MB)\r\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 3.2 MB/s \r\n",
            "\u001b[?25hCollecting tensorflow==2.1.0\r\n",
            "  Downloading tensorflow-2.1.0-cp37-cp37m-manylinux2010_x86_64.whl (421.8 MB)\r\n",
            "\u001b[K     |████████████████████████████████| 421.8 MB 22 kB/s \r\n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /opt/conda/lib/python3.7/site-packages (from ktrain) (1.4.1)\r\n",
            "Collecting scikit-learn==0.21.3\r\n",
            "  Downloading scikit_learn-0.21.3-cp37-cp37m-manylinux1_x86_64.whl (6.7 MB)\r\n",
            "\u001b[K     |████████████████████████████████| 6.7 MB 40.2 MB/s \r\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from ktrain) (3.2.1)\r\n",
            "Requirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from ktrain) (1.0.3)\r\n",
            "Requirement already satisfied: fastprogress>=0.1.21 in /opt/conda/lib/python3.7/site-packages (from ktrain) (0.2.3)\r\n",
            "Collecting keras_bert>=0.81.0\r\n",
            "  Downloading keras-bert-0.85.0.tar.gz (26 kB)\r\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from ktrain) (2.23.0)\r\n",
            "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from ktrain) (0.14.1)\r\n",
            "Collecting langdetect\r\n",
            "  Downloading langdetect-1.0.8.tar.gz (981 kB)\r\n",
            "\u001b[K     |████████████████████████████████| 981 kB 7.3 MB/s \r\n",
            "\u001b[?25hRequirement already satisfied: jieba in /opt/conda/lib/python3.7/site-packages (from ktrain) (0.42.1)\r\n",
            "Collecting cchardet==2.1.5\r\n",
            "  Downloading cchardet-2.1.5-cp37-cp37m-manylinux1_x86_64.whl (241 kB)\r\n",
            "\u001b[K     |████████████████████████████████| 241 kB 39.9 MB/s \r\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.3 in /opt/conda/lib/python3.7/site-packages (from ktrain) (2.4)\r\n",
            "Requirement already satisfied: bokeh in /opt/conda/lib/python3.7/site-packages (from ktrain) (2.1.1)\r\n",
            "Collecting seqeval\r\n",
            "  Downloading seqeval-0.0.12.tar.gz (21 kB)\r\n",
            "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from ktrain) (20.1)\r\n",
            "Requirement already satisfied: tensorflow_datasets in /opt/conda/lib/python3.7/site-packages (from ktrain) (3.1.0)\r\n",
            "Requirement already satisfied: transformers>=2.11.0 in /opt/conda/lib/python3.7/site-packages (from ktrain) (2.11.0)\r\n",
            "Requirement already satisfied: ipython in /opt/conda/lib/python3.7/site-packages (from ktrain) (7.13.0)\r\n",
            "Collecting syntok\r\n",
            "  Downloading syntok-1.3.1.tar.gz (23 kB)\r\n",
            "Collecting whoosh\r\n",
            "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\r\n",
            "\u001b[K     |████████████████████████████████| 468 kB 48.3 MB/s \r\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->ktrain) (0.9.0)\r\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->ktrain) (0.34.2)\r\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\r\n",
            "  Downloading tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\r\n",
            "\u001b[K     |████████████████████████████████| 448 kB 40.8 MB/s \r\n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->ktrain) (3.12.2)\r\n",
            "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->ktrain) (1.14.0)\r\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->ktrain) (1.18.5)\r\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\r\n",
            "  Downloading tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\r\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 25.2 MB/s \r\n",
            "\u001b[?25hCollecting astor>=0.6.0\r\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\r\n",
            "Collecting gast==0.2.2\r\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\r\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->ktrain) (1.1.0)\r\n",
            "Collecting keras-applications>=1.0.8\r\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\r\n",
            "\u001b[K     |████████████████████████████████| 50 kB 4.7 MB/s \r\n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->ktrain) (0.2.0)\r\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->ktrain) (1.30.0)\r\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->ktrain) (3.2.1)\r\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->ktrain) (1.11.2)\r\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0->ktrain) (1.1.2)\r\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.0->ktrain) (0.10.0)\r\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.0->ktrain) (1.2.0)\r\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.0->ktrain) (2.8.1)\r\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.0->ktrain) (2.4.7)\r\n",
            "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.0.1->ktrain) (2019.3)\r\n",
            "Requirement already satisfied: Keras>=2.4.3 in /opt/conda/lib/python3.7/site-packages (from keras_bert>=0.81.0->ktrain) (2.4.3)\r\n",
            "Collecting keras-transformer>=0.38.0\r\n",
            "  Downloading keras-transformer-0.38.0.tar.gz (11 kB)\r\n",
            "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->ktrain) (2.9)\r\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->ktrain) (3.0.4)\r\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->ktrain) (1.24.3)\r\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->ktrain) (2020.6.20)\r\n",
            "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.3->ktrain) (4.4.2)\r\n",
            "Requirement already satisfied: pillow>=4.0 in /opt/conda/lib/python3.7/site-packages (from bokeh->ktrain) (7.2.0)\r\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from bokeh->ktrain) (3.7.4.1)\r\n",
            "Requirement already satisfied: PyYAML>=3.10 in /opt/conda/lib/python3.7/site-packages (from bokeh->ktrain) (5.3.1)\r\n",
            "Requirement already satisfied: Jinja2>=2.7 in /opt/conda/lib/python3.7/site-packages (from bokeh->ktrain) (2.11.2)\r\n",
            "Collecting tornado>=5.1\r\n",
            "  Downloading tornado-6.0.4.tar.gz (496 kB)\r\n",
            "\u001b[K     |████████████████████████████████| 496 kB 46.1 MB/s \r\n",
            "\u001b[?25hRequirement already satisfied: promise in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets->ktrain) (2.3)\r\n",
            "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets->ktrain) (0.18.2)\r\n",
            "Requirement already satisfied: attrs>=18.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets->ktrain) (19.3.0)\r\n",
            "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets->ktrain) (4.45.0)\r\n",
            "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets->ktrain) (0.3.2)\r\n",
            "Requirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets->ktrain) (0.22.2)\r\n",
            "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from transformers>=2.11.0->ktrain) (0.1.91)\r\n",
            "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers>=2.11.0->ktrain) (0.0.43)\r\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers>=2.11.0->ktrain) (3.0.10)\r\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers>=2.11.0->ktrain) (2020.4.4)\r\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /opt/conda/lib/python3.7/site-packages (from transformers>=2.11.0->ktrain) (0.7.0)\r\n",
            "Requirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.7/site-packages (from ipython->ktrain) (0.15.2)\r\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.7/site-packages (from ipython->ktrain) (4.8.0)\r\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython->ktrain) (3.0.5)\r\n",
            "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython->ktrain) (46.1.3.post20200325)\r\n",
            "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython->ktrain) (2.6.1)\r\n",
            "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython->ktrain) (4.3.3)\r\n",
            "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython->ktrain) (0.1.0)\r\n",
            "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython->ktrain) (0.7.5)\r\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (1.0.1)\r\n",
            "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (3.2.1)\r\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (1.14.0)\r\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (0.4.1)\r\n",
            "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==2.1.0->ktrain) (2.10.0)\r\n",
            "Collecting keras-pos-embd>=0.11.0\r\n",
            "  Downloading keras-pos-embd-0.11.0.tar.gz (5.9 kB)\r\n",
            "Collecting keras-multi-head>=0.27.0\r\n",
            "  Downloading keras-multi-head-0.27.0.tar.gz (14 kB)\r\n",
            "Collecting keras-layer-normalization>=0.14.0\r\n",
            "  Downloading keras-layer-normalization-0.14.0.tar.gz (4.3 kB)\r\n",
            "Collecting keras-position-wise-feed-forward>=0.6.0\r\n",
            "  Downloading keras-position-wise-feed-forward-0.6.0.tar.gz (4.4 kB)\r\n",
            "Collecting keras-embed-sim>=0.8.0\r\n",
            "  Downloading keras-embed-sim-0.8.0.tar.gz (4.1 kB)\r\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from Jinja2>=2.7->bokeh->ktrain) (1.1.1)\r\n",
            "Requirement already satisfied: googleapis-common-protos in /opt/conda/lib/python3.7/site-packages (from tensorflow-metadata->tensorflow_datasets->ktrain) (1.51.0)\r\n",
            "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers>=2.11.0->ktrain) (7.1.1)\r\n",
            "Requirement already satisfied: parso>=0.5.2 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.10->ipython->ktrain) (0.5.2)\r\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython->ktrain) (0.6.0)\r\n",
            "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ktrain) (0.1.9)\r\n",
            "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.7/site-packages (from traitlets>=4.2->ipython->ktrain) (0.2.0)\r\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (4.0)\r\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (0.2.7)\r\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (3.1.1)\r\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (1.2.0)\r\n",
            "Collecting keras-self-attention==0.46.0\r\n",
            "  Downloading keras-self-attention-0.46.0.tar.gz (10 kB)\r\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (0.4.8)\r\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (3.0.1)\r\n",
            "Building wheels for collected packages: ktrain, keras-bert, langdetect, seqeval, syntok, gast, keras-transformer, tornado, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\r\n",
            "  Building wheel for ktrain (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n",
            "\u001b[?25h  Created wheel for ktrain: filename=ktrain-0.18.4-py3-none-any.whl size=25253171 sha256=214c942c6dfe040841d2fc9e35516423ed58d5e749759640197e21e203f9186f\r\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/98/d9/a96450b37bdacc871c00a6da8ec6cecf9a6d166b299d7ee3b5\r\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
            "\u001b[?25h  Created wheel for keras-bert: filename=keras_bert-0.85.0-py3-none-any.whl size=34302 sha256=7e5471e71ae8341a062584d37b8425b27c6993f1839b47e7bb46b0d189f2ad6e\r\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/c4/e0/d17f7e6c449d951e6494ad4596b58a6196e31135a88a5e4c51\r\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n",
            "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.8-py3-none-any.whl size=993191 sha256=76be9aa4134536dc5aad45ba20e0dbaa18b48649d1a01c0ff20e8157120c3db2\r\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/f6/9d/85068904dba861c0b9af74e286265a08da438748ee5ae56067\r\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l-\b \bdone\r\n",
            "\u001b[?25h  Created wheel for seqeval: filename=seqeval-0.0.12-py3-none-any.whl size=7423 sha256=a5889da457791240c2c14c3b2d1c6fa725880adbe8cad881e0b0c7545e65d7bf\r\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/cc/62/a3b81f92d35a80e39eb9b2a9d8b31abac54c02b21b2d466edc\r\n",
            "  Building wheel for syntok (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
            "\u001b[?25h  Created wheel for syntok: filename=syntok-1.3.1-py3-none-any.whl size=20916 sha256=8143e7d24963f3d97160f4561ba84ef4993ae87269221258add631737b64d641\r\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/c2/33/e5d7d8f2f8b0c391d76bf82b844c3151bf23a84d75d02b185f\r\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
            "\u001b[?25h  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7539 sha256=79cf6862083733e95165280cfeb4d344d16551b39ce0281feb26acec55e63abe\r\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\r\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
            "\u001b[?25h  Created wheel for keras-transformer: filename=keras_transformer-0.38.0-py3-none-any.whl size=12941 sha256=6074d8e376df09f8483578000c18de4267cbb8c6ea1a383d11d8fb43f2d87f7e\r\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/67/58/bcfb43b6ab764496a446021a8d05991adffd48c16582381326\r\n",
            "  Building wheel for tornado (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n",
            "\u001b[?25h  Created wheel for tornado: filename=tornado-6.0.4-cp37-cp37m-linux_x86_64.whl size=428635 sha256=d10abe105b3bf8117b0b4c1d8a1d7f26a421fc0c3147f616dbee08c30f461519\r\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/14/fa/d88fb5da77d813ea0ffca38a2ab2a052874e9e1142bad0b348\r\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
            "\u001b[?25h  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-py3-none-any.whl size=7553 sha256=f22f5f4d9ec41b4681db86b89d706615284091005ec8d5ac9723f40b95fd64b2\r\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/66/e9/c7eafddc29b81a98786f12b48a2aee7e3c633f6bf4a62cbc20\r\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
            "\u001b[?25h  Created wheel for keras-multi-head: filename=keras_multi_head-0.27.0-py3-none-any.whl size=15613 sha256=843c2a8277513a32fd8a52e43d542ea2e6fcbd01f7508419751cfe78d71a3cb0\r\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/38/cc/50e6d62d6d458e8223d3ddaef7c622b67ae57708193918051b\r\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
            "\u001b[?25h  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-py3-none-any.whl size=5267 sha256=0919b56532c59b37cab5c414c4f2b66ad109ba3cc17d22c912d449cc68ffcece\r\n",
            "  Stored in directory: /root/.cache/pip/wheels/58/14/24/76b0d99b7d9cc17e110956e0fae825a5da3e70a60273220502\r\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
            "\u001b[?25h  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-py3-none-any.whl size=5623 sha256=c86172c938f734f9668ea3d32d4f55593d91c7940efe24a293f25644145a29fd\r\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/53/a2/651c985b605e6a6c48688c779808eb1956fabb910b0557d871\r\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
            "\u001b[?25h  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.8.0-py3-none-any.whl size=4558 sha256=133486d2fd434d3ab83bd835b27f62e2710941f7b9f268e8a1e828abb216acd6\r\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/31/2c/2d3fb4442f6112b92cd56bf801ff25421f302c755f935d4a79\r\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
            "\u001b[?25h  Created wheel for keras-self-attention: filename=keras_self_attention-0.46.0-py3-none-any.whl size=17278 sha256=37a9bbc73e49dd462b5095b305c42c7f49262a0ebb795d439360b0b760640fed\r\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/f7/48/30de93f8333298bad9202aab9b04db0cfd58dcd379b5a5ef1c\r\n",
            "Successfully built ktrain keras-bert langdetect seqeval syntok gast keras-transformer tornado keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\r\n",
            "\u001b[31mERROR: tpot 0.11.5 has requirement scikit-learn>=0.22.0, but you'll have scikit-learn 0.21.3 which is incompatible.\u001b[0m\r\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\r\n",
            "\u001b[31mERROR: kmeans-smote 0.1.2 has requirement imbalanced-learn<0.5,>=0.4.0, but you'll have imbalanced-learn 0.7.0 which is incompatible.\u001b[0m\r\n",
            "\u001b[31mERROR: kmeans-smote 0.1.2 has requirement numpy<1.16,>=1.13, but you'll have numpy 1.18.5 which is incompatible.\u001b[0m\r\n",
            "\u001b[31mERROR: kmeans-smote 0.1.2 has requirement scikit-learn<0.21,>=0.19.0, but you'll have scikit-learn 0.21.3 which is incompatible.\u001b[0m\r\n",
            "\u001b[31mERROR: jupyterlab-git 0.10.0 has requirement nbdime<2.0.0,>=1.1.0, but you'll have nbdime 2.0.0 which is incompatible.\u001b[0m\r\n",
            "\u001b[31mERROR: imbalanced-learn 0.7.0 has requirement scikit-learn>=0.23, but you'll have scikit-learn 0.21.3 which is incompatible.\u001b[0m\r\n",
            "\u001b[31mERROR: dask-ml 1.5.0 has requirement scikit-learn>=0.23, but you'll have scikit-learn 0.21.3 which is incompatible.\u001b[0m\r\n",
            "\u001b[31mERROR: cesium 0.9.12 has requirement scikit-learn>=0.22.1, but you'll have scikit-learn 0.21.3 which is incompatible.\u001b[0m\r\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, astor, gast, keras-applications, tensorflow, scikit-learn, keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert, langdetect, cchardet, seqeval, syntok, whoosh, ktrain, tornado\r\n",
            "  Attempting uninstall: tensorflow-estimator\r\n",
            "    Found existing installation: tensorflow-estimator 2.2.0\r\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\r\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\r\n",
            "  Attempting uninstall: tensorboard\r\n",
            "    Found existing installation: tensorboard 2.2.2\r\n",
            "    Uninstalling tensorboard-2.2.2:\r\n",
            "      Successfully uninstalled tensorboard-2.2.2\r\n",
            "  Attempting uninstall: gast\r\n",
            "    Found existing installation: gast 0.3.3\r\n",
            "    Uninstalling gast-0.3.3:\r\n",
            "      Successfully uninstalled gast-0.3.3\r\n",
            "  Attempting uninstall: tensorflow\r\n",
            "    Found existing installation: tensorflow 2.2.0\r\n",
            "    Uninstalling tensorflow-2.2.0:\r\n",
            "      Successfully uninstalled tensorflow-2.2.0\r\n",
            "  Attempting uninstall: scikit-learn\r\n",
            "    Found existing installation: scikit-learn 0.23.1\r\n",
            "    Uninstalling scikit-learn-0.23.1:\r\n",
            "      Successfully uninstalled scikit-learn-0.23.1\r\n",
            "  Attempting uninstall: tornado\r\n",
            "    Found existing installation: tornado 5.0.2\r\n",
            "    Uninstalling tornado-5.0.2:\r\n",
            "      Successfully uninstalled tornado-5.0.2\r\n",
            "Successfully installed astor-0.8.1 cchardet-2.1.5 gast-0.2.2 keras-applications-1.0.8 keras-bert-0.85.0 keras-embed-sim-0.8.0 keras-layer-normalization-0.14.0 keras-multi-head-0.27.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.46.0 keras-transformer-0.38.0 ktrain-0.18.4 langdetect-1.0.8 scikit-learn-0.21.3 seqeval-0.0.12 syntok-1.3.1 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0 tornado-6.0.4 whoosh-2.7.4\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8lwTCvX4abU",
        "colab_type": "code",
        "colab": {},
        "outputId": "23d71678-f5be-4cbf-b4c5-3059e22c6127"
      },
      "source": [
        "!git clone https://github.com/sarthak-sriw/IMDB-Movie-Reviews-Large-Dataset-50k.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'IMDB-Movie-Reviews-Large-Dataset-50k'...\r\n",
            "remote: Enumerating objects: 10, done.\u001b[K\r\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\r\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\r\n",
            "remote: Total 10 (delta 1), reused 0 (delta 0), pack-reused 0\u001b[K\r\n",
            "Unpacking objects: 100% (10/10), done.\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GepWKkM4aVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# /content/IMDB-Movie-Reviews-Large-Dataset-50k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YW_Jxjqf4aSJ",
        "colab_type": "code",
        "colab": {},
        "outputId": "623a9dbe-e195-44f6-a589-c2182a80bb3b"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ktrain\n",
        "from ktrain import text\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYCOq0GB0edr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_test = pd.read_excel('./IMDB-Movie-Reviews-Large-Dataset-50k/train.xlsx', dtype= str)\n",
        "data_train = pd.read_excel('./IMDB-Movie-Reviews-Large-Dataset-50k/test.xlsx', dtype = str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPffRgyM1Vc9",
        "colab_type": "code",
        "colab": {},
        "outputId": "de57bfe8-7405-4a89-c7d3-d0639c46511d"
      },
      "source": [
        "data_train.sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Reviews</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18490</th>\n",
              "      <td>This movie is bizarre. Better put, it's \"freak...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20313</th>\n",
              "      <td>Hobgoblins... what a concept. Rick Sloan was a...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7200</th>\n",
              "      <td>Wonderful film, one of the best horror films o...</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10818</th>\n",
              "      <td>I may be getting ahead of myself here, but alt...</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12539</th>\n",
              "      <td>This film is wonderful in every way that moder...</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 Reviews Sentiment\n",
              "18490  This movie is bizarre. Better put, it's \"freak...       neg\n",
              "20313  Hobgoblins... what a concept. Rick Sloan was a...       neg\n",
              "7200   Wonderful film, one of the best horror films o...       pos\n",
              "10818  I may be getting ahead of myself here, but alt...       pos\n",
              "12539  This film is wonderful in every way that moder...       pos"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnXGDGYE1Yn5",
        "colab_type": "code",
        "colab": {},
        "outputId": "ee43d994-4976-49e9-e61b-b34f6efe9a6e"
      },
      "source": [
        "text.print_text_classifiers()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fasttext: a fastText-like model [http://arxiv.org/pdf/1607.01759.pdf]\n",
            "logreg: logistic regression using a trainable Embedding layer\n",
            "nbsvm: NBSVM model [http://www.aclweb.org/anthology/P12-2018]\n",
            "bigru: Bidirectional GRU with pretrained fasttext word vectors [https://fasttext.cc/docs/en/crawl-vectors.html]\n",
            "standard_gru: simple 2-layer GRU with randomly initialized embeddings\n",
            "bert: Bidirectional Encoder Representations from Transformers (BERT) [https://arxiv.org/abs/1810.04805]\n",
            "distilbert: distilled, smaller, and faster BERT from Hugging Face [https://arxiv.org/abs/1910.01108]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArjDvkJn1pwp",
        "colab_type": "code",
        "colab": {
          "referenced_widgets": [
            "99e246b3e4fb46cf863e5e1bb198df22"
          ]
        },
        "outputId": "8c87b766-6e04-4ff4-ef7d-8989da1451fb"
      },
      "source": [
        "(train, val, preproc) = text.texts_from_df(train_df=data_train, text_column='Reviews', label_columns='Sentiment',\n",
        "                   val_df = data_test,\n",
        "                   maxlen = 400,\n",
        "                   preprocess_mode = 'distilbert')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preprocessing train...\n",
            "language: en\n",
            "train sequence lengths:\n",
            "\tmean : 234\n",
            "\t95percentile : 598\n",
            "\t99percentile : 913\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99e246b3e4fb46cf863e5e1bb198df22",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Is Multi-Label? False\n",
            "preprocessing test...\n",
            "language: en\n",
            "test sequence lengths:\n",
            "\tmean : 234\n",
            "\t95percentile : 598\n",
            "\t99percentile : 913\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnwYNxJb2XIm",
        "colab_type": "code",
        "colab": {
          "referenced_widgets": [
            "3f0b5411ea2842ccad689fca7401cd75",
            "0218f3cad37246229e67e6300e905d4b"
          ]
        },
        "outputId": "206512a7-5081-4f4f-bcc6-cb0cd92d040e"
      },
      "source": [
        "model = text.text_classifier(name = 'distilbert', train_data = train, preproc=preproc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is Multi-Label? False\n",
            "maxlen is 400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3f0b5411ea2842ccad689fca7401cd75",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0218f3cad37246229e67e6300e905d4b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=363423424.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26QFg7VV4nvG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner = ktrain.get_learner(model = model,\n",
        "                             train_data = train,\n",
        "                             val_data = val,\n",
        "                             batch_size = 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNcVJ__v4nqE",
        "colab_type": "code",
        "colab": {},
        "outputId": "1ec845b3-838e-42b9-d3c4-42c624b3295b"
      },
      "source": [
        "learner.fit_onecycle(lr = 2e-5, epochs=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "begin training using onecycle policy with max lr of 2e-05...\n",
            "Train for 4167 steps, validate for 782 steps\n",
            "Epoch 1/2\n",
            "4167/4167 [==============================] - 1016s 244ms/step - loss: 0.2938 - accuracy: 0.8736 - val_loss: 0.1630 - val_accuracy: 0.9428\n",
            "Epoch 2/2\n",
            "4167/4167 [==============================] - 997s 239ms/step - loss: 0.1592 - accuracy: 0.9400 - val_loss: 0.0637 - val_accuracy: 0.9828\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc8832f00d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rW8bsakl4nl6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictor = ktrain.get_predictor(learner.model, preproc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xI0NpQN4nf4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictor.save('./')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnqXw9CA4ndM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBN4Jk8u4naa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = ['this movie was really bad. acting was also bad. I will not watch again',\n",
        "        'the movie was really great. I will see it again', 'another great movie. must watch to everyone']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkJWZAXk4nXM",
        "colab_type": "code",
        "colab": {},
        "outputId": "13748802-de9c-4ba7-f04e-327ec256af75"
      },
      "source": [
        "predictor.predict(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['neg', 'pos', 'pos']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oiO-7qy26Zp",
        "colab_type": "code",
        "colab": {},
        "outputId": "494fd588-f0e9-4481-ca7c-f81d6996a4b7"
      },
      "source": [
        "predictor.get_classes()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['neg', 'pos']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl8bGAUoW-A7",
        "colab_type": "code",
        "colab": {},
        "outputId": "1370b791-c2c4-4fb8-e58d-cc25d92ca73b"
      },
      "source": [
        "predictor.predict(data, return_proba=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.994142  , 0.00585801],\n",
              "       [0.00469813, 0.99530184],\n",
              "       [0.00349588, 0.9965042 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34y6Ud0QXL0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}